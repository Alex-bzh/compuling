{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tokenisation\n",
    "\n",
    "Processus de segmentation d’un texte en unités (tokens) significatives (phrases, mots, symboles…).\n",
    "\n",
    "Souvent la première étape avant toute analyse de texte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Préparation\n",
    "\n",
    "1. Importer NLTK\n",
    "2. Installer le module `punkt` (si besoin)\n",
    "3. Charger le tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Segmentation en phrases\n",
    "\n",
    "Méthode `sent_tokenize()`\n",
    "\n",
    "Exemple avec deux extraits issus des *Adventures of Buster Bear* (Burgess, 1920) et du *Cid* (Corneille, 1637) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "en = \"Little Joe sat down on the bank and prepared to enjoy his breakfast. He hadn't seen Buster Bear, and he didn't know that he or any one else was anywhere near.\"\n",
    "fr = \"Elvire, m’as-tu fait un rapport bien sincère ? Ne déguises-tu rien de ce qu’a dit mon père ?\"\n",
    "sent_tokenize(en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pour plus d’efficacité avec des textes longs, charger directement le fichier *pickle* (outil pour sérialiser un objet en Python)\n",
    "\n",
    "Et sélectionner celui qui convient à la langue de travail :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/PY3/french.pickle')\n",
    "tokenizer.tokenize(fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Segmentation en mots\n",
    "\n",
    "Méthode `word_tokenize()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(word_tokenize(en))\n",
    "print(word_tokenize(fr, language='french'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Remarques :\n",
    "- ponctuation conservée\n",
    "- contractions séparées (en anglais)\n",
    "\n",
    "Segmentation selon espaces et ponctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Méthode `word_tokenize()` fait par défaut appel à une classe `TreebankWordTokenizer`.\n",
    "\n",
    "D’autres *tokenizers* existent :\n",
    "- `WhitespaceTokenizer`\n",
    "- `SpaceTokenizer`\n",
    "- `WordPunctTokenizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Comparons les différences :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer, SpaceTokenizer, WordPunctTokenizer\n",
    "sentence = \"He hadn't seen Buster Bear, and he didn't know that he or any one else was anywhere near.\"\n",
    "tokenizers = [WhitespaceTokenizer(), SpaceTokenizer(), WordPunctTokenizer()]\n",
    "print([tokenizer.tokenize(sentence) for tokenizer in tokenizers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Les contractions et la ponctuation sont gérées différemment :\n",
    "\n",
    "| Tokenizer    | Contraction      | Ponctuation |\n",
    "|--------------|:----------------:|:------------|\n",
    "| TreebankWord | *had*, *n't*     | excluse     |\n",
    "| Whitespace   | *hadn't*         | incluse     |\n",
    "| Space        | *hadn't*         | incluse     |\n",
    "| WordPunct    | *hadn*, *'*, *t* | excluse     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Une autre classe de tokenizer permet de personnaliser son modèle de tokenisation : `RegexpTokenizer`\n",
    "\n",
    "Utile par exemple pour supprimer toute ponctuation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "print(tokenizer.tokenize(fr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Et pour inclure les contractions de l’anglais ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "print(tokenizer.tokenize(en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Par défaut, `RegexpTokenizer` travaille sur les tokens, mais on peut lui demander de travailler plutôt sur les espaces :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\s]+\", gaps=True)\n",
    "print(tokenizer.tokenize(en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Une autre opération courante consiste à ne conserver que les mots signifiants d’un texte.\n",
    "\n",
    "NLTK fournit une liste de mots vides pour plusieurs langues :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Sample of stopwords in French\n",
    "print(stopwords.words('french')[::4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Et pour filtrer grâce à cette liste :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "tokenizer = RegexpTokenizer(\"\\w+\")\n",
    "fr_stopwords = stopwords.words('french')\n",
    "words = tokenizer.tokenize(fr)\n",
    "print([word for word in words if word not in fr_stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application\n",
    "\n",
    "Affichez une liste triée par fréquence des mots présents dans *Moby Dick* de Herman Melville.\n",
    "\n",
    "**Note :** le texte est disponible dans le corpus Gutenberg de NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, unicodedata, string\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "\n",
    "def remove_accent_mark(word):\n",
    "    \"\"\"Removes the accent marks on a character.\n",
    "    \n",
    "    Argument keyword:\n",
    "    word -- the word to analyse\n",
    "    \"\"\"\n",
    "    word_norm = \"\"\n",
    "    for character in word:\n",
    "        char_norm = unicodedata.normalize('NFKD', character)\n",
    "        word_norm += char_norm[0]\n",
    "    return word_norm\n",
    "\n",
    "words = gutenberg.words('melville-moby_dick.txt')\n",
    "occurrences = {}\n",
    "for word in words:\n",
    "    if word not in stopwords.words('english') + list(string.punctuation):\n",
    "        occurrences.update({\n",
    "            word: occurrences.get(word, 0) + 1\n",
    "        })\n",
    "\n",
    "words_sorted = list(occurrences.items())\n",
    "words_sorted = sorted(words_sorted, key=lambda this:remove_accent_mark(this[0].lower()))\n",
    "words_sorted = sorted(words_sorted, key=lambda this:this[1], reverse=True)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
